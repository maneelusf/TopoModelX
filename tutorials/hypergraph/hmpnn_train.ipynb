{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from topomodelx.nn.hypergraph.hmpnn_layer import HMPNNLayer\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(\".\", \"cora\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the incidence matrix ($B_1$) which is of shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"incidence_1\"] = torch.sparse_coo_tensor(\n",
    "    dataset[\"edge_index\"], torch.ones(dataset[\"edge_index\"].shape[1]), dtype=torch.long\n",
    ")\n",
    "dataset = dataset.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the `HMPNNLayer` class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(41)\n",
    "\n",
    "in_features = 1433\n",
    "hidden_features = 8\n",
    "num_classes = 7\n",
    "n_layers = 2\n",
    "\n",
    "model = HMPNN(in_features, (256, hidden_features), num_classes, n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train loss: 2.1079 train acc: 0.14 val loss: 2.1436 val acc: 0.16\n",
      "Epoch: 2 train loss: 2.0234 train acc: 0.15 val loss: 2.1016 val acc: 0.16\n",
      "Epoch: 3 train loss: 1.9800 train acc: 0.15 val loss: 2.0681 val acc: 0.16\n",
      "Epoch: 4 train loss: 1.9504 train acc: 0.18 val loss: 2.0389 val acc: 0.16\n",
      "Epoch: 5 train loss: 1.9194 train acc: 0.21 val loss: 2.0137 val acc: 0.16\n",
      "Epoch: 6 train loss: 1.9241 train acc: 0.19 val loss: 1.9917 val acc: 0.16\n",
      "Epoch: 7 train loss: 1.8917 train acc: 0.19 val loss: 1.9729 val acc: 0.16\n",
      "Epoch: 8 train loss: 1.8710 train acc: 0.23 val loss: 1.9556 val acc: 0.16\n",
      "Epoch: 9 train loss: 1.8574 train acc: 0.29 val loss: 1.9402 val acc: 0.17\n",
      "Epoch: 10 train loss: 1.8646 train acc: 0.29 val loss: 1.9265 val acc: 0.17\n",
      "Epoch: 11 train loss: 1.8540 train acc: 0.33 val loss: 1.9136 val acc: 0.19\n",
      "Epoch: 12 train loss: 1.8430 train acc: 0.36 val loss: 1.9012 val acc: 0.22\n",
      "Epoch: 13 train loss: 1.8336 train acc: 0.38 val loss: 1.8886 val acc: 0.25\n",
      "Epoch: 14 train loss: 1.8405 train acc: 0.34 val loss: 1.8775 val acc: 0.30\n",
      "Epoch: 15 train loss: 1.8264 train acc: 0.34 val loss: 1.8668 val acc: 0.34\n",
      "Epoch: 16 train loss: 1.8065 train acc: 0.38 val loss: 1.8562 val acc: 0.37\n",
      "Epoch: 17 train loss: 1.8158 train acc: 0.37 val loss: 1.8456 val acc: 0.38\n",
      "Epoch: 18 train loss: 1.7957 train acc: 0.44 val loss: 1.8346 val acc: 0.39\n",
      "Epoch: 19 train loss: 1.8028 train acc: 0.40 val loss: 1.8249 val acc: 0.41\n",
      "Epoch: 20 train loss: 1.7882 train acc: 0.37 val loss: 1.8156 val acc: 0.42\n",
      "Epoch: 21 train loss: 1.7912 train acc: 0.36 val loss: 1.8070 val acc: 0.42\n",
      "Epoch: 22 train loss: 1.7610 train acc: 0.46 val loss: 1.7987 val acc: 0.41\n",
      "Epoch: 23 train loss: 1.7617 train acc: 0.47 val loss: 1.7905 val acc: 0.42\n",
      "Epoch: 24 train loss: 1.7596 train acc: 0.41 val loss: 1.7830 val acc: 0.41\n",
      "Epoch: 25 train loss: 1.7391 train acc: 0.38 val loss: 1.7740 val acc: 0.42\n",
      "Epoch: 26 train loss: 1.7315 train acc: 0.44 val loss: 1.7655 val acc: 0.42\n",
      "Epoch: 27 train loss: 1.7365 train acc: 0.42 val loss: 1.7565 val acc: 0.43\n",
      "Epoch: 28 train loss: 1.7184 train acc: 0.48 val loss: 1.7459 val acc: 0.44\n",
      "Epoch: 29 train loss: 1.7085 train acc: 0.44 val loss: 1.7367 val acc: 0.45\n",
      "Epoch: 30 train loss: 1.6815 train acc: 0.47 val loss: 1.7279 val acc: 0.46\n",
      "Epoch: 31 train loss: 1.6673 train acc: 0.50 val loss: 1.7178 val acc: 0.46\n",
      "Epoch: 32 train loss: 1.6846 train acc: 0.51 val loss: 1.7077 val acc: 0.46\n",
      "Epoch: 33 train loss: 1.6483 train acc: 0.51 val loss: 1.7000 val acc: 0.47\n",
      "Epoch: 34 train loss: 1.6436 train acc: 0.54 val loss: 1.6971 val acc: 0.45\n",
      "Epoch: 35 train loss: 1.6353 train acc: 0.54 val loss: 1.6991 val acc: 0.43\n",
      "Epoch: 36 train loss: 1.6336 train acc: 0.54 val loss: 1.6982 val acc: 0.43\n",
      "Epoch: 37 train loss: 1.5938 train acc: 0.60 val loss: 1.6980 val acc: 0.43\n",
      "Epoch: 38 train loss: 1.5886 train acc: 0.56 val loss: 1.6979 val acc: 0.41\n",
      "Epoch: 39 train loss: 1.5974 train acc: 0.55 val loss: 1.6881 val acc: 0.42\n",
      "Epoch: 40 train loss: 1.5600 train acc: 0.52 val loss: 1.6694 val acc: 0.43\n",
      "Epoch: 41 train loss: 1.5445 train acc: 0.64 val loss: 1.6513 val acc: 0.45\n",
      "Epoch: 42 train loss: 1.5501 train acc: 0.60 val loss: 1.6308 val acc: 0.46\n",
      "Epoch: 43 train loss: 1.5397 train acc: 0.54 val loss: 1.6141 val acc: 0.47\n",
      "Epoch: 44 train loss: 1.5096 train acc: 0.59 val loss: 1.6020 val acc: 0.48\n",
      "Epoch: 45 train loss: 1.4992 train acc: 0.59 val loss: 1.5915 val acc: 0.47\n",
      "Epoch: 46 train loss: 1.5020 train acc: 0.58 val loss: 1.5829 val acc: 0.49\n",
      "Epoch: 47 train loss: 1.4710 train acc: 0.64 val loss: 1.5747 val acc: 0.49\n",
      "Epoch: 48 train loss: 1.4608 train acc: 0.67 val loss: 1.5703 val acc: 0.47\n",
      "Epoch: 49 train loss: 1.4341 train acc: 0.62 val loss: 1.5632 val acc: 0.49\n",
      "Epoch: 50 train loss: 1.4428 train acc: 0.66 val loss: 1.5630 val acc: 0.49\n",
      "Epoch: 51 train loss: 1.4209 train acc: 0.66 val loss: 1.5502 val acc: 0.48\n",
      "Epoch: 52 train loss: 1.4151 train acc: 0.63 val loss: 1.5303 val acc: 0.50\n",
      "Epoch: 53 train loss: 1.4090 train acc: 0.66 val loss: 1.5051 val acc: 0.50\n",
      "Epoch: 54 train loss: 1.4021 train acc: 0.64 val loss: 1.4893 val acc: 0.51\n",
      "Epoch: 55 train loss: 1.3847 train acc: 0.65 val loss: 1.4842 val acc: 0.50\n",
      "Epoch: 56 train loss: 1.3907 train acc: 0.61 val loss: 1.4849 val acc: 0.50\n",
      "Epoch: 57 train loss: 1.3434 train acc: 0.70 val loss: 1.4866 val acc: 0.50\n",
      "Epoch: 58 train loss: 1.3253 train acc: 0.69 val loss: 1.4864 val acc: 0.50\n",
      "Epoch: 59 train loss: 1.3380 train acc: 0.66 val loss: 1.4896 val acc: 0.50\n",
      "Epoch: 60 train loss: 1.2933 train acc: 0.69 val loss: 1.4921 val acc: 0.50\n",
      "Epoch: 61 train loss: 1.3124 train acc: 0.68 val loss: 1.4948 val acc: 0.50\n",
      "Epoch: 62 train loss: 1.3091 train acc: 0.65 val loss: 1.4931 val acc: 0.50\n",
      "Epoch: 63 train loss: 1.2768 train acc: 0.67 val loss: 1.4881 val acc: 0.51\n",
      "Epoch: 64 train loss: 1.2749 train acc: 0.65 val loss: 1.4827 val acc: 0.50\n",
      "Epoch: 65 train loss: 1.2740 train acc: 0.69 val loss: 1.4833 val acc: 0.50\n",
      "Epoch: 66 train loss: 1.2773 train acc: 0.67 val loss: 1.4744 val acc: 0.50\n",
      "Epoch: 67 train loss: 1.2430 train acc: 0.67 val loss: 1.4646 val acc: 0.50\n",
      "Epoch: 68 train loss: 1.2020 train acc: 0.69 val loss: 1.4648 val acc: 0.49\n",
      "Epoch: 69 train loss: 1.1958 train acc: 0.69 val loss: 1.4734 val acc: 0.48\n",
      "Epoch: 70 train loss: 1.1895 train acc: 0.71 val loss: 1.4748 val acc: 0.47\n",
      "Epoch: 71 train loss: 1.2008 train acc: 0.70 val loss: 1.4760 val acc: 0.47\n",
      "Epoch: 72 train loss: 1.1573 train acc: 0.74 val loss: 1.4385 val acc: 0.50\n",
      "Epoch: 73 train loss: 1.1751 train acc: 0.69 val loss: 1.4242 val acc: 0.50\n",
      "Epoch: 74 train loss: 1.1889 train acc: 0.73 val loss: 1.4183 val acc: 0.51\n",
      "Epoch: 75 train loss: 1.1762 train acc: 0.72 val loss: 1.4250 val acc: 0.51\n",
      "Epoch: 76 train loss: 1.1737 train acc: 0.66 val loss: 1.4471 val acc: 0.51\n",
      "Epoch: 77 train loss: 1.1242 train acc: 0.73 val loss: 1.4559 val acc: 0.49\n",
      "Epoch: 78 train loss: 1.0648 train acc: 0.76 val loss: 1.4498 val acc: 0.51\n",
      "Epoch: 79 train loss: 1.0717 train acc: 0.76 val loss: 1.4506 val acc: 0.50\n",
      "Epoch: 80 train loss: 1.0568 train acc: 0.79 val loss: 1.4410 val acc: 0.50\n",
      "Epoch: 81 train loss: 1.0650 train acc: 0.73 val loss: 1.4375 val acc: 0.50\n",
      "Epoch: 82 train loss: 1.0475 train acc: 0.74 val loss: 1.4441 val acc: 0.51\n",
      "Epoch: 83 train loss: 1.0293 train acc: 0.78 val loss: 1.4450 val acc: 0.51\n",
      "Epoch: 84 train loss: 1.0494 train acc: 0.74 val loss: 1.4622 val acc: 0.51\n",
      "Epoch: 85 train loss: 1.0200 train acc: 0.74 val loss: 1.4609 val acc: 0.51\n",
      "Epoch: 86 train loss: 1.0358 train acc: 0.76 val loss: 1.4645 val acc: 0.51\n",
      "Epoch: 87 train loss: 0.9877 train acc: 0.80 val loss: 1.4748 val acc: 0.50\n",
      "Epoch: 88 train loss: 1.0173 train acc: 0.75 val loss: 1.4788 val acc: 0.49\n",
      "Epoch: 89 train loss: 0.9744 train acc: 0.78 val loss: 1.4970 val acc: 0.48\n",
      "Epoch: 90 train loss: 0.9497 train acc: 0.84 val loss: 1.4981 val acc: 0.49\n",
      "Epoch: 91 train loss: 0.9345 train acc: 0.79 val loss: 1.4736 val acc: 0.49\n",
      "Epoch: 92 train loss: 0.9636 train acc: 0.81 val loss: 1.4339 val acc: 0.51\n",
      "Epoch: 93 train loss: 0.9197 train acc: 0.78 val loss: 1.4162 val acc: 0.50\n",
      "Epoch: 94 train loss: 0.8984 train acc: 0.80 val loss: 1.3813 val acc: 0.53\n",
      "Epoch: 95 train loss: 0.8895 train acc: 0.85 val loss: 1.3547 val acc: 0.54\n",
      "Epoch: 96 train loss: 0.9048 train acc: 0.81 val loss: 1.3499 val acc: 0.54\n",
      "Epoch: 97 train loss: 0.8737 train acc: 0.82 val loss: 1.3502 val acc: 0.54\n",
      "Epoch: 98 train loss: 0.9479 train acc: 0.78 val loss: 1.3518 val acc: 0.54\n",
      "Epoch: 99 train loss: 0.8906 train acc: 0.80 val loss: 1.3639 val acc: 0.54\n",
      "Epoch: 100 train loss: 0.8589 train acc: 0.85 val loss: 1.3789 val acc: 0.53\n"
     ]
    }
   ],
   "source": [
    "train_y_true = dataset[\"y\"][dataset[\"train_mask\"]]\n",
    "val_y_true = dataset[\"y\"][dataset[\"val_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    loss = loss_fn(y_pred_logits[dataset[\"train_mask\"]], train_y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(train_y_true, y_pred[dataset[\"train_mask\"]])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "    val_loss = loss_fn(y_pred_logits[dataset[\"val_mask\"]], val_y_true).item()\n",
    "    y_pred = y_pred_logits.argmax(dim=-1)\n",
    "    val_acc = accuracy_score(val_y_true, y_pred[dataset[\"val_mask\"]])\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "        f\"val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2982 test acc: 0.55 \n"
     ]
    }
   ],
   "source": [
    "test_y_true = dataset[\"y\"][dataset[\"test_mask\"]]\n",
    "initial_x_1 = torch.zeros_like(dataset[\"x\"])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(dataset[\"x\"], initial_x_1, dataset[\"incidence_1\"])\n",
    "test_loss = loss_fn(y_pred_logits[dataset[\"test_mask\"]], test_y_true).item()\n",
    "y_pred = y_pred_logits.argmax(dim=-1)\n",
    "test_acc = accuracy_score(test_y_true, y_pred[dataset[\"test_mask\"]])\n",
    "print(f\"Test loss: {test_loss:.4f} test acc: {test_acc:.2f} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
