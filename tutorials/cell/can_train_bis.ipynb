{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Cellular Attention Network (CAN)\n",
    "\n",
    "We create and train a neural network for cellular complexes with layers using a message passing scheme provided by the down and up Laplacians as proposed in [Rodenberry et. al: Signal processing on cell complexes (2022)](https://arxiv.org/pdf/2110.05614.pdf). We also train layers utilizing the cell attention mechanism originally proposed in [Giusti et. al: Cell Attention Networks (2022)](https://arxiv.org/abs/2209.08179). \n",
    "\n",
    "\n",
    "<!-- We create and train a simplified version of the CCXN originally proposed in [Hajij et. al : Cell Complex Neural Networks (2020)](https://arxiv.org/pdf/2010.00743.pdf). -->\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The equations of one layer of this neural network without the attention mechanism are given by:\n",
    "\n",
    "- A convolution from edges to edges using the down and up laplacian to pass messages:\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 0 \\rightarrow 1)} = L_{\\downarrow,1} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 0 \\rightarrow 1)}$ \n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 2 \\rightarrow 1)} = L_{\\uparrow,1} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 2 \\rightarrow 1)}$ \n",
    "\n",
    "游린 $\\quad m_{x \\rightarrow x}^{(1 \\rightarrow 1)}  = h_x^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 1)}$ \n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 0 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 0 \\rightarrow 1)}$ \n",
    "\n",
    "游릲 $\\quad m_x^{(1 \\rightarrow 2 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 2 \\rightarrow 1)}$ \n",
    "\n",
    "游릴: $\\quad m_x^{(1)} = m_x^{(1 \\rightarrow 0 \\rightarrow 1)} + m_{x \\rightarrow x}^{(1 \\rightarrow 1)} +m_x^{(1 \\rightarrow 2 \\rightarrow 1)}$ \n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(1)} = \\sigma(m_{x}^{(1)})$\n",
    "\n",
    "\n",
    "\n",
    "The equations of one layer of this neural network with the attention mechanism are given by: masked by the up and down Laplacian are given by:\n",
    "\n",
    "- A convolution from edges to edges using an attention mechanism masked by the down and up Laplacians:\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 2 \\rightarrow 1)} = (L_{\\uparrow,1} \\odot att(h_{y \\in \\mathcal{L}\\uparrow(x)}^{t,(1)}, h_x^{t,(1)}))_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 2 \\rightarrow 1)} $ \n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 0 \\rightarrow 1)} = (L_{\\downarrow,1} \\odot att(h_{y \\in \\mathcal{L}\\downarrow(x)}^{t,(1)}, h_x^{t,(1)}))_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1 \\rightarrow 0 \\rightarrow 1)}$ \n",
    "\n",
    "游린 $\\quad m^{(1 \\rightarrow 1)}_{x \\rightarrow x} = (1+\\epsilon)\\cdot h_x^{t, (1)} \\cdot \\Theta^{t,(1 \\rightarrow 1)} $ \n",
    "\n",
    "游릲 $\\quad m_{x}^{(1 \\rightarrow 2 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{L}_\\uparrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 2 \\rightarrow 1)}$ \n",
    "\n",
    "游릲 $\\quad m_{x}^{(1 \\rightarrow 0 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{(1 \\rightarrow 0 \\rightarrow 1)}$ \n",
    "\n",
    "游릲 $\\quad m^{(1 \\rightarrow 1)}_{x} = m^{(1 \\rightarrow 1)}_{x \\rightarrow x}$ \n",
    "\n",
    "游릴 $\\quad m_x^{(1)} = m_x^{(1 \\rightarrow 1)} + m_{x}^{(1 \\rightarrow 2 \\rightarrow 1)} + m_{x}^{(1 \\rightarrow 0 \\rightarrow 1)}$ \n",
    "\n",
    "游릱 $\\quad h_x^{t+1, (1)} = \\sigma(\\theta_{att} \\cdot m_x^{(1)})\\cdot \\sigma(m_x^{(1)})$\n",
    "\n",
    "\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031).\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on a small version of [shrec16](http://shapenet.cs.stanford.edu/shrec16/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import toponetx.datasets as datasets\n",
    "\n",
    "from topomodelx.nn.cell.can_layer_bis import CANLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a cell complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0`,`x_1`, and `x_2` on the nodes (0-cells), edges (1-cells), and faces (2-cells) for each complex: these will be the model's inputs,\n",
    "- a binary classification label `y` associated to the cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th simplicial complex has 252 nodes with features of dimension 6.\n",
      "The 6th simplicial complex has 750 edges with features of dimension 10.\n",
      "The 6th simplicial complex has 500 faces with features of dimension 7.\n",
      "The 6th simplicial complex has label 9.\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_0s[i_complex].shape[0]} nodes with features of dimension {x_0s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_1s[i_complex].shape[0]} edges with features of dimension {x_1s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_2s[i_complex].shape[0]} faces with features of dimension {x_2s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(f\"The {i_complex}th simplicial complex has label {ys[i_complex]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift into cell complex domain and define neighborhood structures\n",
    "\n",
    "We lift each simplicial complex into a cell complex.\n",
    "\n",
    "Then, we retrieve the neighborhood structures (i.e. their representative matrices) taht we will use to send messages on each cell complex. In th case of this architecture we need the down and up laplacians acting on 1-cells denoted by $L_{\\downarrow,1}, L_{\\uparrow,1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_list = []\n",
    "down_laplacian_list = []\n",
    "up_laplacian_list = []\n",
    "for simplex in simplexes:\n",
    "    cell_complex = simplex.to_cell_complex()\n",
    "    cc_list.append(cell_complex)\n",
    "\n",
    "    down_laplacian = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    up_laplacian = cell_complex.up_laplacian_matrix(rank=1)\n",
    "    down_laplacian = torch.from_numpy(down_laplacian.todense()).to_sparse()\n",
    "    up_laplacian = torch.from_numpy(up_laplacian.todense()).to_sparse()\n",
    "    down_laplacian_list.append(down_laplacian)\n",
    "    up_laplacian_list.append(up_laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th cell complex has a down_laplacian matrix of shape torch.Size([750, 750]).\n",
      "The 6th cell complex has an up_laplacian matrix of shape torch.Size([750, 750]).\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th cell complex has a down_laplacian matrix of shape {down_laplacian_list[i_complex].shape}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th cell complex has an up_laplacian matrix of shape {up_laplacian_list[i_complex].shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Implementing the CAN architecture will require to perform message passing along neighborhood structures of the cell complexes.\n",
    "\n",
    "Thus, now we retrieve these neighborhood structures (i.e. their representative matrices) that we will use to send messages. \n",
    "\n",
    "For the CAN, we need the down Laplacian matrix $L_{\\downarrow, 1}$ and the up Laplacian matrix $L_{\\uparrow,1}$ of each cell complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up Laplacian of the 0-th complex: torch.Size([750, 750]).\n",
      "Down Laplacian of the 0-th complex: torch.Size([750, 750]).\n"
     ]
    }
   ],
   "source": [
    "up_laplacian_list = []\n",
    "down_laplacian_list = []\n",
    "for cell_complex in cc_list:\n",
    "    up_laplacian = cell_complex.up_laplacian_matrix(rank=1)\n",
    "    down_laplacian = cell_complex.down_laplacian_matrix(rank=1)\n",
    "    up_laplacian = torch.from_numpy(up_laplacian.todense()).to_sparse()\n",
    "    down_laplacian = torch.from_numpy(down_laplacian.todense()).to_sparse()\n",
    "    up_laplacian_list.append(up_laplacian)\n",
    "    down_laplacian_list.append(down_laplacian)\n",
    "\n",
    "i_cc = 0\n",
    "print(f\"Up Laplacian of the {i_cc}-th complex: {up_laplacian_list[i_cc].shape}.\")\n",
    "print(f\"Down Laplacian of the {i_cc}-th complex: {down_laplacian_list[i_cc].shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the CANLayer class, we create a neural network which applies a CAN layer to the edges followed by linear layers on nodes, edges, and faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of input features on nodes, edges and faces are: 6, 10 and 7.\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0s[0].shape[-1]\n",
    "in_channels_1 = x_1s[0].shape[-1]\n",
    "in_channels_2 = x_2s[0].shape[-1]\n",
    "print(\n",
    "    f\"The dimension of input features on nodes, edges and faces are: {in_channels_0}, {in_channels_1} and {in_channels_2}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAN(torch.nn.Module):\n",
    "    \"\"\"CAN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels_0 : int\n",
    "        Dimension of input features on nodes.\n",
    "    in_channels_1 : int\n",
    "        Dimension of input features on edges.\n",
    "    in_channels_2 : int\n",
    "        Dimension of input features on faces.\n",
    "    num_classes : int\n",
    "        Number of classes.\n",
    "    n_layers : int\n",
    "        Number of CAN layers.\n",
    "    att : bool\n",
    "        Whether to use attention.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_0,\n",
    "        in_channels_1,\n",
    "        in_channels_2,\n",
    "        num_classes,\n",
    "        n_layers=2,\n",
    "        att=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(CANLayer(channels=in_channels_1, att=att))\n",
    "        self.layers = layers\n",
    "        self.lin_0 = torch.nn.Linear(in_channels_0, num_classes)\n",
    "        self.lin_1 = torch.nn.Linear(in_channels_1, num_classes)\n",
    "        self.lin_2 = torch.nn.Linear(in_channels_2, num_classes)\n",
    "\n",
    "    def forward(self, x_0, x_1, x_2, down_laplacian, up_laplacian):\n",
    "        \"\"\"Forward computation through layers, then linear layers, then avg pooling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_0 : torch.Tensor, shape = [n_nodes, in_channels_0]\n",
    "            Input features on the nodes (0-cells).\n",
    "        x_1 : torch.Tensor, shape = [n_edges, in_channels_1]\n",
    "            Input features on the edges (1-cells).\n",
    "        x_2 : torch.Tensor, shape = [n_faces, in_channels_2]\n",
    "            Input features on the faces (2-cells).\n",
    "        down_laplacian : tensor, shape = [n_edges, n_edges]\n",
    "            Down Laplacian of rank 1.\n",
    "        up_laplacian : tensor, shape = [n_edges, n_edges]\n",
    "            Up Laplacian of rank 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _ : tensor, shape = [1]\n",
    "            Label assigned to whole complex.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_1 = layer(x_1, down_laplacian, up_laplacian)\n",
    "        x_0 = self.lin_0(x_0)\n",
    "        x_1 = self.lin_1(x_1)\n",
    "        x_2 = self.lin_2(x_2)\n",
    "        # Take the average of the 2D, 1D and 0D cell features. If they are NaN, convert them to 0.\n",
    "        two_dimensional_cells_mean = torch.nanmean(x_2, dim=0)\n",
    "        two_dimensional_cells_mean[torch.isnan(two_dimensional_cells_mean)] = 0\n",
    "        one_dimensional_cells_mean = torch.nanmean(x_1, dim=0)\n",
    "        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] = 0\n",
    "        zero_dimensional_cells_mean = torch.nanmean(x_0, dim=0)\n",
    "        zero_dimensional_cells_mean[torch.isnan(zero_dimensional_cells_mean)] = 0\n",
    "        # Return the sum of the averages\n",
    "        return (\n",
    "            one_dimensional_cells_mean\n",
    "            + zero_dimensional_cells_mean\n",
    "            + two_dimensional_cells_mean\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, initialize loss, and specify an optimizer. We first try it without any attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAN(in_channels_0, in_channels_1, in_channels_2, num_classes=1, n_layers=2)\n",
    "model = model.to(device)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, shuffle=False)\n",
    "x_2_train, x_2_test = train_test_split(x_2s, test_size=test_size, shuffle=False)\n",
    "up_laplacian_train, up_laplacian_test = train_test_split(\n",
    "    up_laplacian_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "down_laplacian_train, down_laplacian_test = train_test_split(\n",
    "    down_laplacian_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the CAN using 10 epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrah\\anaconda3\\envs\\topological_2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 89.3527\n",
      "Epoch: 2 loss: 81.9469\n",
      "Test_loss: 59.5947\n",
      "Epoch: 3 loss: 81.1082\n",
      "Epoch: 4 loss: 80.3476\n",
      "Test_loss: 55.3188\n",
      "Epoch: 5 loss: 79.6493\n",
      "Epoch: 6 loss: 79.0137\n",
      "Test_loss: 51.6529\n",
      "Epoch: 7 loss: 78.4377\n",
      "Epoch: 8 loss: 77.9167\n",
      "Test_loss: 48.4984\n",
      "Epoch: 9 loss: 77.4454\n",
      "Epoch: 10 loss: 77.0191\n",
      "Test_loss: 45.7851\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 10\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, x_1, x_2, down_laplacian, up_laplacian, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        x_2_train,\n",
    "        down_laplacian_train,\n",
    "        up_laplacian_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0, x_1, x_2, y = (\n",
    "            torch.tensor(x_0).float().to(device),\n",
    "            torch.tensor(x_1).float().to(device),\n",
    "            torch.tensor(x_2).float().to(device),\n",
    "            torch.tensor(y).float().to(device),\n",
    "        )\n",
    "        down_laplacian, up_laplacian = down_laplacian.float().to(\n",
    "            device\n",
    "        ), up_laplacian.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, x_2, down_laplacian, up_laplacian)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x_0, x_1, x_2, down_laplacian, up_laplcian, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                x_2_test,\n",
    "                down_laplacian_test,\n",
    "                up_laplacian_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0, x_1, x_2, y = (\n",
    "                    torch.tensor(x_0).float().to(device),\n",
    "                    torch.tensor(x_1).float().to(device),\n",
    "                    torch.tensor(x_2).float().to(device),\n",
    "                    torch.tensor(y).float().to(device),\n",
    "                )\n",
    "                down_laplacian, up_laplacian = down_laplacian.float().to(\n",
    "                    device\n",
    "                ), up_laplacian.float().to(device)\n",
    "                y_hat = model(x_0, x_1, x_2, down_laplacian, up_laplacian)\n",
    "                test_loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {test_loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network with Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new neural network, that uses the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAN(\n",
    "    in_channels_0, in_channels_1, in_channels_2, num_classes=1, n_layers=2, att=True\n",
    ")\n",
    "model = model.to(device)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the training for this neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 96.6442\n",
      "Epoch: 2 loss: 76.8734\n",
      "Test_loss: 52.6675\n",
      "Epoch: 3 loss: 76.2222\n",
      "Epoch: 4 loss: 75.4701\n",
      "Test_loss: 46.0780\n",
      "Epoch: 5 loss: 74.7502\n",
      "Epoch: 6 loss: 74.0797\n",
      "Test_loss: 40.4300\n",
      "Epoch: 7 loss: 73.4617\n",
      "Epoch: 8 loss: 72.8948\n",
      "Test_loss: 35.5483\n",
      "Epoch: 9 loss: 72.3760\n",
      "Epoch: 10 loss: 71.9013\n",
      "Test_loss: 31.3371\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 10\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, x_1, x_2, down_laplacian, up_laplacian, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        x_2_train,\n",
    "        down_laplacian_train,\n",
    "        up_laplacian_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0, x_1, x_2, y = (\n",
    "            torch.tensor(x_0).float().to(device),\n",
    "            torch.tensor(x_1).float().to(device),\n",
    "            torch.tensor(x_2).float().to(device),\n",
    "            torch.tensor(y).float().to(device),\n",
    "        )\n",
    "        down_laplacian, up_laplacian = down_laplacian.float().to(\n",
    "            device\n",
    "        ), up_laplacian.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, x_2, down_laplacian, up_laplacian)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x_0, x_1, x_2, down_laplacian, up_laplcian, y in zip(\n",
    "                x_0_test,\n",
    "                x_1_test,\n",
    "                x_2_test,\n",
    "                down_laplacian_test,\n",
    "                up_laplacian_test,\n",
    "                y_test,\n",
    "            ):\n",
    "                x_0, x_1, x_2, y = (\n",
    "                    torch.tensor(x_0).float().to(device),\n",
    "                    torch.tensor(x_1).float().to(device),\n",
    "                    torch.tensor(x_2).float().to(device),\n",
    "                    torch.tensor(y).float().to(device),\n",
    "                )\n",
    "                down_laplacian, up_laplacian = down_laplacian.float().to(\n",
    "                    device\n",
    "                ), up_laplacian.float().to(device)\n",
    "                y_hat = model(x_0, x_1, x_2, down_laplacian, up_laplacian)\n",
    "                test_loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {test_loss:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
