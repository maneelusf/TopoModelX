{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)\n",
    "\n",
    "\n",
    "ðŸŸ¥ $\\quad m_{y \\rightarrow x}^{(r \\rightarrow r'' \\rightarrow r)} = M(h_{x}^{t, (r)}, h_{y}^{t, (r)},att(h_{x}^{t, (r)}, h_{y}^{t, (r)}),x,y,{\\Theta^t}) \\qquad \\text{where } r'' < r < r'$\n",
    "\n",
    "ðŸŸ¥ $\\quad m_{y \\rightarrow x}^{(r'' \\rightarrow r)} = M(h_{x}^{t, (r)}, h_{y}^{t, (r'')},att(h_{x}^{t, (r)}, h_{y}^{t, (r'')}),x,y,{\\Theta^t})$\n",
    "\n",
    "ðŸŸ§ $\\quad m_x^{(r \\rightarrow r)}  = AGG_{y \\in \\mathcal{L}\\_\\downarrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "ðŸŸ§ $\\quad m_x^{(r'' \\rightarrow r)} = AGG_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r'' \\rightarrow r)}$\n",
    "\n",
    "ðŸŸ© $\\quad m_x^{(r)}  = \\text{AGG}\\_{\\mathcal{N}\\_k \\in \\mathcal{N}}(m_x^{(k)})$\n",
    "\n",
    "ðŸŸ¦ $\\quad h_{x}^{t+1, (r)} = U(h_x^{t, (r)}, m_{x}^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import toponetx.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from topomodelx.nn.simplicial.sca_cmps import SCACMPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPUs are available we will make use of them. Otherwise, we will use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a simplicial complex.\n",
    "\n",
    "We also retrieve:\n",
    "- input signals `x_0`, `x_1`, and `x_2` on the nodes (0-cells), edges (1-cells), and faces (2-cells) for each complex: these will be the model's inputs,\n",
    "- a scalar classification label `y` associated to the simplicial complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "scs = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th simplicial complex has 252 nodes with features of dimension 6.\n",
      "The 6th simplicial complex has 750 edges with features of dimension 10.\n",
      "The 6th simplicial complex has 500 faces with features of dimension 7.\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_0s[i_complex].shape[0]} nodes with features of dimension {x_0s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_1s[i_complex].shape[0]} edges with features of dimension {x_1s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_2s[i_complex].shape[0]} faces with features of dimension {x_2s[i_complex].shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the inputs to test each message passing scheme:\n",
    "\n",
    "#### Coadjacency Message Passing Scheme (CMPS):\n",
    "This will require features from faces, and edges again, but outputs features on the edges. The first neighborhood matrix will be the level 2 lower Laplacian, $L_{\\downarrow, 2}$, and the second neighborhood matrix will be the transpose of the incidence matrix of the faces, $B_{2}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_down_1_list = []\n",
    "laplacian_down_2_list = []\n",
    "incidence1_t_list = []\n",
    "incidence2_t_list = []\n",
    "\n",
    "for sc in scs:\n",
    "    laplacian_down_1 = sc.down_laplacian_matrix(rank=1)\n",
    "    laplacian_down_2 = sc.down_laplacian_matrix(rank=2)\n",
    "    incidence_1_t = sc.incidence_matrix(rank=1).T\n",
    "    incidence_2_t = sc.incidence_matrix(rank=2).T\n",
    "\n",
    "    laplacian_down_1 = torch.from_numpy(laplacian_down_1.todense()).to_sparse()\n",
    "    laplacian_down_2 = torch.from_numpy(laplacian_down_2.todense()).to_sparse()\n",
    "    incidence_1_t = torch.from_numpy(incidence_1_t.todense()).to_sparse()\n",
    "    incidence_2_t = torch.from_numpy(incidence_2_t.todense()).to_sparse()\n",
    "\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_down_2_list.append(laplacian_down_2)\n",
    "    incidence1_t_list.append(incidence_1_t)\n",
    "    incidence2_t_list.append(incidence_2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Networks\n",
    "\n",
    "Using the SCACMPSLayer class, we create a neural network with a modifiable number of layers each following the CMPS at each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_list = [x_0s[0].shape[-1], x_1s[0].shape[-1], x_2s[0].shape[-1]]\n",
    "\n",
    "complex_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, shuffle=False)\n",
    "x_2_train, x_2_test = train_test_split(x_2s, test_size=test_size, shuffle=False)\n",
    "\n",
    "laplacian_down_1_train, laplacian_down_1_test = train_test_split(\n",
    "    laplacian_down_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "laplacian_down_2_train, laplacian_down_2_test = train_test_split(\n",
    "    laplacian_down_2_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "incidence1_t_train, incidence1_t_test = train_test_split(\n",
    "    incidence1_t_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "incidence2_t_train, incidence2_t_test = train_test_split(\n",
    "    incidence2_t_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Model\n",
    "Because the SCA implementation in [HZPMC22]_ was used for clustering, we did the same in one dimension to train on the int classification labels provided my the shrec16 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCACMPS(\n",
    "    channels_list=channels_list,\n",
    "    complex_dim=complex_dim,\n",
    "    n_classes=1,\n",
    "    n_layers=3,\n",
    "    att=False,\n",
    ")\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
