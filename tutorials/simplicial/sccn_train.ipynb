{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Convolutional Network (SCCN)\n",
    "\n",
    "We create a SCCN model a la [Yang et al : Efficient Representation Learning for Higher-Order Data with\n",
    "Simplicial Complexes (LoG 2022)](https://proceedings.mlr.press/v198/yang22a/yang22a.pdf)\n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The model operates on cells of all ranks up to some max rank $r_\\mathrm{max}$.\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r \\rightarrow r)} = (H_{r})_{xy} \\cdot h^{t,(r)}_y \\cdot \\Theta^{t,(r\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r-1 \\rightarrow r)} = (B_{r}^T)_{xy} \\cdot h^{t,(r-1)}_y \\cdot \\Theta^{t,(r-1\\to r)}$,    (for $1\\leq r \\leq r_\\mathrm{max}$)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow x}}^{(r+1 \\rightarrow r)} = (B_{r+1})_{xy} \\cdot h^{t,(r+1)}_y \\cdot \\Theta^{t,(r+1\\to r)}$,    (for $0\\leq r \\leq r_\\mathrm{max}-1$)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r \\rightarrow r)}  = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)\\bigcup \\mathcal{L}_\\uparrow(x)} m_{y \\rightarrow x}^{(r \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r-1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(r-1 \\rightarrow r)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(r+1 \\rightarrow r)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(r)}  = m_x^{(r \\rightarrow r)} + m_x^{(r-1 \\rightarrow r)} + m_x^{(r+1 \\rightarrow r)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(r)}  = \\sigma(m_x^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccn import SCCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain.\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. The feature dimension is `feat_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=8)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Our implementation allows for features on cells up to an arbitrary maximum rank. In this dataset, we can use at most `max_rank = 3`, which is what we choose.\n",
    "\n",
    "We define incidence and adjacency matrices up to the max rank and put them in dictionaries indexed by the rank, as is expected by the `SCCNLayer`.\n",
    "The form of tha adjacency and incidence matrices could be chosen arbitrarily, here we follow the original formulation by Yang et al. quite closely and select the adjacencies as r-Hodge Laplacians $H_r$, summed with $2I$ (or just $I$ for $r\\in\\{0, r_\\mathrm{max}\\}$) to allow cells to pass messages to themselves. The incidence matrices are the usual boundary matrices $B_r$.\n",
    "One could additionally weight/normalize these matrices as suggested by Yang et al., but we refrain from doing this for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 3  # There are features up to tetrahedron order in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix H0 has shape: torch.Size([34, 34]).\n",
      "The adjacency matrix H1 has shape: torch.Size([78, 78]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix H2 has shape: torch.Size([45, 45]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The adjacency matrix H3 has shape: torch.Size([11, 11]).\n",
      "The incidence matrix B3 has shape: torch.Size([45, 11]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninamiolane/opt/anaconda3/envs/tmx/lib/python3.11/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def sparse_to_torch(X):\n",
    "    return torch.from_numpy(X.todense()).to_sparse()\n",
    "\n",
    "\n",
    "incidences = {\n",
    "    f\"rank_{r}\": sparse_to_torch(dataset.incidence_matrix(rank=r))\n",
    "    for r in range(1, max_rank + 1)\n",
    "}\n",
    "\n",
    "adjacencies = {}\n",
    "adjacencies[\"rank_0\"] = (\n",
    "    sparse_to_torch(dataset.adjacency_matrix(rank=0))\n",
    "    + torch.eye(dataset.shape[0]).to_sparse()\n",
    ")\n",
    "for r in range(1, max_rank):\n",
    "    adjacencies[f\"rank_{r}\"] = (\n",
    "        sparse_to_torch(\n",
    "            dataset.adjacency_matrix(rank=r) + dataset.coadjacency_matrix(rank=r)\n",
    "        )\n",
    "        + 2 * torch.eye(dataset.shape[r]).to_sparse()\n",
    "    )\n",
    "adjacencies[f\"rank_{max_rank}\"] = (\n",
    "    sparse_to_torch(dataset.coadjacency_matrix(rank=max_rank))\n",
    "    + torch.eye(dataset.shape[max_rank]).to_sparse()\n",
    ")\n",
    "\n",
    "for r in range(max_rank + 1):\n",
    "    print(f\"The adjacency matrix H{r} has shape: {adjacencies[f'rank_{r}'].shape}.\")\n",
    "    if r > 0:\n",
    "        print(f\"The incidence matrix B{r} has shape: {incidences[f'rank_{r}'].shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "We import the features at each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = torch.tensor(np.stack(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"tetrahedron_feat\").items():\n",
    "    x_3.append(v)\n",
    "x_3 = torch.tensor(np.stack(x_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tetrahedrons with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"There are {x_3.shape[0]} tetrahedrons with features of dimension {x_3.shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are organized in a dictionary keeping track of their rank, similar to the adjacencies/incidences earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\"rank_0\": x_0, \"rank_1\": x_1, \"rank_2\": x_2, \"rank_3\": x_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train = torch.from_numpy(y[:30])\n",
    "y_test = torch.from_numpy(y[30:])\n",
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes}$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCCN(\n",
    "    channels=channels_nodes,\n",
    "    max_rank=max_rank,\n",
    "    n_layers=5,\n",
    "    update_func=\"sigmoid\",\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs. Typically achieves 100% train accuracy. Test accuracy is more arbitrary between runs, likely due to the small dataset set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.6721 Train_acc: 0.6333\n",
      "Epoch: 2 loss: 0.6891 Train_acc: 0.5667\n",
      "Epoch: 3 loss: 0.6284 Train_acc: 0.5667\n",
      "Epoch: 4 loss: 0.6173 Train_acc: 0.6667\n",
      "Epoch: 5 loss: 0.6110 Train_acc: 0.7000\n",
      "Epoch: 6 loss: 0.5831 Train_acc: 0.7000\n",
      "Epoch: 7 loss: 0.5695 Train_acc: 0.7000\n",
      "Epoch: 8 loss: 0.5638 Train_acc: 0.7000\n",
      "Epoch: 9 loss: 0.5493 Train_acc: 0.7333\n",
      "Epoch: 10 loss: 0.5384 Train_acc: 0.7667\n",
      "Epoch: 11 loss: 0.5141 Train_acc: 0.7333\n",
      "Epoch: 12 loss: 0.5201 Train_acc: 0.6667\n",
      "Epoch: 13 loss: 0.5201 Train_acc: 0.7000\n",
      "Epoch: 14 loss: 0.5038 Train_acc: 0.6667\n",
      "Epoch: 15 loss: 0.5016 Train_acc: 0.7333\n",
      "Epoch: 16 loss: 0.4906 Train_acc: 0.7333\n",
      "Epoch: 17 loss: 0.4763 Train_acc: 0.7000\n",
      "Epoch: 18 loss: 0.4545 Train_acc: 0.7667\n",
      "Epoch: 19 loss: 0.4483 Train_acc: 0.7667\n",
      "Epoch: 20 loss: 0.4153 Train_acc: 0.8000\n",
      "Epoch: 21 loss: 0.4062 Train_acc: 0.8000\n",
      "Epoch: 22 loss: 0.3790 Train_acc: 0.8333\n",
      "Epoch: 23 loss: 0.3916 Train_acc: 0.7667\n",
      "Epoch: 24 loss: 0.3529 Train_acc: 0.8667\n",
      "Epoch: 25 loss: 0.2900 Train_acc: 0.8667\n",
      "Epoch: 26 loss: 0.2359 Train_acc: 0.9333\n",
      "Epoch: 27 loss: 0.2002 Train_acc: 0.9667\n",
      "Epoch: 28 loss: 0.2970 Train_acc: 0.9000\n",
      "Epoch: 29 loss: 0.2032 Train_acc: 0.9333\n",
      "Epoch: 30 loss: 0.2329 Train_acc: 0.9000\n",
      "Epoch: 31 loss: 0.1913 Train_acc: 0.8667\n",
      "Epoch: 32 loss: 0.1516 Train_acc: 0.9667\n",
      "Epoch: 33 loss: 0.1348 Train_acc: 0.9667\n",
      "Epoch: 34 loss: 0.1257 Train_acc: 0.9333\n",
      "Epoch: 35 loss: 0.1283 Train_acc: 0.9667\n",
      "Epoch: 36 loss: 0.1136 Train_acc: 0.9333\n",
      "Epoch: 37 loss: 0.0851 Train_acc: 0.9667\n",
      "Epoch: 38 loss: 0.2019 Train_acc: 0.8667\n",
      "Epoch: 39 loss: 0.1101 Train_acc: 0.9667\n",
      "Epoch: 40 loss: 0.0873 Train_acc: 0.9667\n",
      "Epoch: 41 loss: 0.0674 Train_acc: 0.9667\n",
      "Epoch: 42 loss: 0.0562 Train_acc: 1.0000\n",
      "Epoch: 43 loss: 0.0529 Train_acc: 1.0000\n",
      "Epoch: 44 loss: 0.0512 Train_acc: 1.0000\n",
      "Epoch: 45 loss: 0.0469 Train_acc: 1.0000\n",
      "Epoch: 46 loss: 0.0363 Train_acc: 1.0000\n",
      "Epoch: 47 loss: 0.0293 Train_acc: 1.0000\n",
      "Epoch: 48 loss: 0.0272 Train_acc: 1.0000\n",
      "Epoch: 49 loss: 0.0264 Train_acc: 1.0000\n",
      "Epoch: 50 loss: 0.0245 Train_acc: 1.0000\n",
      "Test_acc: 0.5000\n",
      "Epoch: 51 loss: 0.0207 Train_acc: 1.0000\n",
      "Epoch: 52 loss: 0.0165 Train_acc: 1.0000\n",
      "Epoch: 53 loss: 0.0132 Train_acc: 1.0000\n",
      "Epoch: 54 loss: 0.0114 Train_acc: 1.0000\n",
      "Epoch: 55 loss: 0.0113 Train_acc: 1.0000\n",
      "Epoch: 56 loss: 0.0117 Train_acc: 1.0000\n",
      "Epoch: 57 loss: 0.0101 Train_acc: 1.0000\n",
      "Epoch: 58 loss: 0.0081 Train_acc: 1.0000\n",
      "Epoch: 59 loss: 0.0071 Train_acc: 1.0000\n",
      "Epoch: 60 loss: 0.0065 Train_acc: 1.0000\n",
      "Epoch: 61 loss: 0.0061 Train_acc: 1.0000\n",
      "Epoch: 62 loss: 0.0057 Train_acc: 1.0000\n",
      "Epoch: 63 loss: 0.0054 Train_acc: 1.0000\n",
      "Epoch: 64 loss: 0.0050 Train_acc: 1.0000\n",
      "Epoch: 65 loss: 0.0046 Train_acc: 1.0000\n",
      "Epoch: 66 loss: 0.0043 Train_acc: 1.0000\n",
      "Epoch: 67 loss: 0.0040 Train_acc: 1.0000\n",
      "Epoch: 68 loss: 0.0038 Train_acc: 1.0000\n",
      "Epoch: 69 loss: 0.0036 Train_acc: 1.0000\n",
      "Epoch: 70 loss: 0.0034 Train_acc: 1.0000\n",
      "Epoch: 71 loss: 0.0033 Train_acc: 1.0000\n",
      "Epoch: 72 loss: 0.0032 Train_acc: 1.0000\n",
      "Epoch: 73 loss: 0.0031 Train_acc: 1.0000\n",
      "Epoch: 74 loss: 0.0030 Train_acc: 1.0000\n",
      "Epoch: 75 loss: 0.0030 Train_acc: 1.0000\n",
      "Epoch: 76 loss: 0.0029 Train_acc: 1.0000\n",
      "Epoch: 77 loss: 0.0028 Train_acc: 1.0000\n",
      "Epoch: 78 loss: 0.0027 Train_acc: 1.0000\n",
      "Epoch: 79 loss: 0.0026 Train_acc: 1.0000\n",
      "Epoch: 80 loss: 0.0026 Train_acc: 1.0000\n",
      "Epoch: 81 loss: 0.0025 Train_acc: 1.0000\n",
      "Epoch: 82 loss: 0.0024 Train_acc: 1.0000\n",
      "Epoch: 83 loss: 0.0024 Train_acc: 1.0000\n",
      "Epoch: 84 loss: 0.0023 Train_acc: 1.0000\n",
      "Epoch: 85 loss: 0.0023 Train_acc: 1.0000\n",
      "Epoch: 86 loss: 0.0023 Train_acc: 1.0000\n",
      "Epoch: 87 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 88 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 89 loss: 0.0022 Train_acc: 1.0000\n",
      "Epoch: 90 loss: 0.0021 Train_acc: 1.0000\n",
      "Epoch: 91 loss: 0.0021 Train_acc: 1.0000\n",
      "Epoch: 92 loss: 0.0021 Train_acc: 1.0000\n",
      "Epoch: 93 loss: 0.0020 Train_acc: 1.0000\n",
      "Epoch: 94 loss: 0.0020 Train_acc: 1.0000\n",
      "Epoch: 95 loss: 0.0020 Train_acc: 1.0000\n",
      "Epoch: 96 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 97 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 98 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 99 loss: 0.0019 Train_acc: 1.0000\n",
      "Epoch: 100 loss: 0.0018 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 101 loss: 0.0018 Train_acc: 1.0000\n",
      "Epoch: 102 loss: 0.0018 Train_acc: 1.0000\n",
      "Epoch: 103 loss: 0.0018 Train_acc: 1.0000\n",
      "Epoch: 104 loss: 0.0018 Train_acc: 1.0000\n",
      "Epoch: 105 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 106 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 107 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 108 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 109 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 110 loss: 0.0017 Train_acc: 1.0000\n",
      "Epoch: 111 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 112 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 113 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 114 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 115 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 116 loss: 0.0016 Train_acc: 1.0000\n",
      "Epoch: 117 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 118 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 119 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 120 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 121 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 122 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 123 loss: 0.0015 Train_acc: 1.0000\n",
      "Epoch: 124 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 125 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 126 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 127 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 128 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 129 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 130 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 131 loss: 0.0014 Train_acc: 1.0000\n",
      "Epoch: 132 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 133 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 134 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 135 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 136 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 137 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 138 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 139 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 140 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 141 loss: 0.0013 Train_acc: 1.0000\n",
      "Epoch: 142 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 143 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 144 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 145 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 146 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 147 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 148 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 149 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 150 loss: 0.0012 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 151 loss: 0.0012 Train_acc: 1.0000\n",
      "Epoch: 152 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 153 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 154 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 155 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 156 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 157 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 158 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 159 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 160 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 161 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 162 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 163 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 164 loss: 0.0011 Train_acc: 1.0000\n",
      "Epoch: 165 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 166 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 167 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 168 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 169 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 170 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 171 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 172 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 173 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 174 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 175 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 176 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 177 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 178 loss: 0.0010 Train_acc: 1.0000\n",
      "Epoch: 179 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 180 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 181 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 182 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 183 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 184 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 185 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 186 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 187 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 188 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 189 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 190 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 191 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 192 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 193 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 194 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 195 loss: 0.0009 Train_acc: 1.0000\n",
      "Epoch: 196 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 197 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 198 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 199 loss: 0.0008 Train_acc: 1.0000\n",
      "Epoch: 200 loss: 0.0008 Train_acc: 1.0000\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 50\n",
    "num_epochs = 200\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(features, incidences, adjacencies)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = (y_hat > 0).long()\n",
    "    accuracy = (y_pred[: len(y_train)] == y_train).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(features, incidences, adjacencies)\n",
    "            y_pred_test = (y_hat_test > 0).long()\n",
    "            test_accuracy = (\n",
    "                (y_pred_test[-len(y_test) :] == y_test).float().mean().item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
